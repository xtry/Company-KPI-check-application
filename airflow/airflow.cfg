[core]
dags_folder = /opt/airflow/dags
executor = CeleryExecutor
sql_alchemy_conn = postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@airflowdb:5432/airflow
parallelism = 32
fernet_key = GmhP3ADRHscUZ2z_ohwMOmXlu5jFSI5IQRG0s-KrV_Y=
default_timezone = Europe/London

[database]
sql_engine_encoding = utf-8

[webserver]
base_url = http://localhost:8080
web_server_port = 8080
web_server_worker_timeout = 120
web_server_master_timeout = 300
webserver_config = ./tmp/webserver_config.py
auth_backend = airflow.contrib.auth.backends.password_auth
secret_key = G8ZbXirwKyqoV6DvfotU-qIQrTB77iW5shFBR4L3PKk
jwt_secret = zKyRQkchRLhbv4DzXXra5Pbq6jJ-3aZG6SmLWqFCSKw

[scheduler]
scheduler_heartbeat_sec = 60
max_active_runs_per_dag = 16
max_active_tasks_per_dag = 16

[logging]
base_log_folder = /opt/airflow/logs
remote_base_log_folder = /opt/airflow/logs
remote_logging = False
log_format = [%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s
log_filename_template = dag_id={{ ti.dag_id }}/run_id={{ ti.run_id }}/task_id={{ ti.task_id }}/%% if ti.map_index >= 0 %%map_index={{ ti.map_index }}/%% endif %%attempt={{ try_number }}.log
log_processor_filename_template = {{ filename }}.log
# Log URL Format
log_url = http://localhost:8080/log/{dag_id}/{task_id}/{execution_date}/{try_number}.log
log_fetcher = http
worker_log_server_port = 8793
logging_level = DEBUG

[celery]
broker_url = redis://redis:6379/0
result_backend = db+postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@airflowdb:5432/airflow